{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script pocesses the MNIST dataset.\n",
    "Original MNIST dataset:\n",
    "- Author: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\n",
    "- License: Creative Commons Attribution-Share Alike 3.0 (CC BY-SA 3.0)\n",
    "- Source: https://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Processed dataset is distributed under th same license (CC BY-SA 3.0).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "\n",
    "# 保存先ディレクトリを設定\n",
    "output_dir = './../resources/MNIST/Qunomon'\n",
    "try:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"ディレクトリの作成に失敗しました: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "train_file_path=\"../resources/MNIST/aug_train.h5\"\n",
    "test_file_path=\"../resources/MNIST/aug_test.h5\"\n",
    "import torch\n",
    "from torch.utils.data import DataLoader ,random_split,TensorDataset\n",
    "import h5py\n",
    "\n",
    "def load_h5_data(h5_filename,x_name,y_name):\n",
    "    \"\"\"\n",
    "    h5形式のデータセットからデータセットを作成する\n",
    "    parameter:\n",
    "        h5_filename:ファイルのパス\n",
    "        x_name:ファイル内にある画像データセットの名前\n",
    "        y_name:ファイル内にある正解ラベルデータセットの名前\n",
    "        batch_size:バッチサイズ\n",
    "    return:\n",
    "        dataset:データ\n",
    "    \n",
    "    \"\"\"\n",
    "    with h5py.File(h5_filename,'r') as h5_file:\n",
    "        images = h5_file[x_name][:]\n",
    "        labels = h5_file[y_name][:]\n",
    "    images = torch.tensor(images,dtype=torch.float32)\n",
    "    labels = torch.tensor(labels,dtype=torch.long)\n",
    "    dataset = TensorDataset(images,labels)\n",
    "    return dataset\n",
    "#データセットの読み込み\n",
    "train_dataset = load_h5_data(train_file_path,'train_image','train_label')\n",
    "test_dataset = load_h5_data(test_file_path,'test_image','test_label')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#層化抽出法でサンプルを抽出する関数\n",
    "def stratified_sample(dataset, sample_size):\n",
    "    labels= [label for _, label in dataset]\n",
    "    strat_split = StratifiedShuffleSplit(n_splits =1, test_size=sample_size,random_state=42)\n",
    "    for _, subset_idx in strat_split.split(np.zeros(len(labels)),labels):\n",
    "        subset = [dataset[i] for i in subset_idx] \n",
    "    return subset\n",
    "#サンプルサイズの設定\n",
    "train_sample_size= 6000\n",
    "test_sample_size = 1000\n",
    "#サンプルの抽出\n",
    "sample_train_data = stratified_sample(train_dataset,train_sample_size)\n",
    "sample_test_data = stratified_sample(test_dataset,test_sample_size)\n",
    "\n",
    "\n",
    "# HDF5に保存\n",
    "def save_to_hdf5_qunomon(dataset, h5_filename,x_name,y_name, target_size=(28, 28)):\n",
    "    file_path = os.path.join(output_dir, h5_filename)\n",
    "    try:\n",
    "        with h5py.File(file_path, 'w') as h5_file:\n",
    "            images = h5_file.create_dataset(x_name, (len(dataset),1, *target_size), dtype='float32',compression='gzip')\n",
    "            labels = h5_file.create_dataset(y_name, (len(dataset),), dtype='int64',compression='gzip')\n",
    "            batch_size =100\n",
    "            image_batch=[]\n",
    "            label_batch =[]\n",
    "            \n",
    "            for i, (image, label) in enumerate(dataset):\n",
    "                image_batch.append(image)\n",
    "                label_batch.append(label)\n",
    "\n",
    "                if (i+1)% batch_size ==0 or (i+1)==len(dataset):\n",
    "                    images[i+1-batch_size:i+1]=np.array(image_batch)\n",
    "                    labels[i+1-batch_size:i+1]=np.array(label_batch)\n",
    "                    image_batch.clear()\n",
    "                    label_batch.clear()\n",
    "                    \n",
    "        # ファイルが存在するか確認\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"データが {file_path} に保存されました。\")  # 成功メッセージ\n",
    "        else:\n",
    "            print(f\"ファイル保存に失敗しました: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_path} の保存中にエラーが発生しました: {e}\")\n",
    "\n",
    "# データを保存\n",
    "save_to_hdf5_qunomon(sample_train_data, 'aug_train.h5','train_image','train_label')\n",
    "save_to_hdf5_qunomon(sample_test_data, 'aug_test.h5','test_image','test_label')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
